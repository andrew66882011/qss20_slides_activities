{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "06_textasdata_partI_textmining_examplecode.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew66882011/qss20_slides_activities/blob/main/activities/06_textasdata_partI_textmining_examplecode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXTBmPRxmWt"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQM4zTQyxmWv",
        "outputId": "251a231c-1b33-475d-f3cd-85567366adad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "## load packages \n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "## nltk imports\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "## uncomment and download if this is your first \n",
        "## time running \n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "## sentiment analysis\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "## specify to print all output in a call\n",
        "## and not just first\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-578381e85824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m## sentiment analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m## specify to print all output in a call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vaderSentiment'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxmOQMMtxmWw"
      },
      "source": [
        "## spacy (still being installed on jhub)\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atoyFvW_xmWw"
      },
      "source": [
        "# Load data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSutE24VxmWw",
        "outputId": "4d1a607c-d431-48ab-b211-88a452e2ca6b"
      },
      "source": [
        "## if working from within the repo, can use this relative path\n",
        "path_todata = \"../public_data/airbnb_text.zip\"\n",
        "\n",
        "## load data\n",
        "ab = pd.read_csv(path_todata)\n",
        "ab.head()\n",
        "ab.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>name_upper</th>\n",
              "      <th>neighbourhood_group</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2539</td>\n",
              "      <td>Clean &amp; quiet apt home by the park</td>\n",
              "      <td>CLEAN &amp; QUIET APT HOME BY THE PARK</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2595</td>\n",
              "      <td>Skylit Midtown Castle</td>\n",
              "      <td>SKYLIT MIDTOWN CASTLE</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3647</td>\n",
              "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
              "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3831</td>\n",
              "      <td>Cozy Entire Floor of Brownstone</td>\n",
              "      <td>COZY ENTIRE FLOOR OF BROWNSTONE</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5022</td>\n",
              "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
              "      <td>ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                              name  \\\n",
              "0  2539                Clean & quiet apt home by the park   \n",
              "1  2595                             Skylit Midtown Castle   \n",
              "2  3647               THE VILLAGE OF HARLEM....NEW YORK !   \n",
              "3  3831                   Cozy Entire Floor of Brownstone   \n",
              "4  5022  Entire Apt: Spacious Studio/Loft by central park   \n",
              "\n",
              "                                         name_upper neighbourhood_group  price  \n",
              "0                CLEAN & QUIET APT HOME BY THE PARK            Brooklyn    149  \n",
              "1                             SKYLIT MIDTOWN CASTLE           Manhattan    225  \n",
              "2               THE VILLAGE OF HARLEM....NEW YORK !           Manhattan    150  \n",
              "3                   COZY ENTIRE FLOOR OF BROWNSTONE            Brooklyn     89  \n",
              "4  ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK           Manhattan     80  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48895 entries, 0 to 48894\n",
            "Data columns (total 5 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   id                   48895 non-null  int64 \n",
            " 1   name                 48879 non-null  object\n",
            " 2   name_upper           48879 non-null  object\n",
            " 3   neighbourhood_group  48895 non-null  object\n",
            " 4   price                48895 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 1.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RmNTCoxmWx"
      },
      "source": [
        "# Text mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK7DnGaTxmWy"
      },
      "source": [
        "## Manual approach 1: look for a single word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1FtdHrixmWy",
        "outputId": "a22db580-f83e-4e27-aa7a-7185afb5f40b"
      },
      "source": [
        "## using the `name_upper` var, look at where reviews mention cozy\n",
        "ab['is_cozy'] = np.where(ab.name_upper.str.contains(\"COZY\"), True, False)\n",
        "\n",
        "## find the mean price by neighborhood and whether cozy\n",
        "mp = pd.DataFrame(ab.groupby(['is_cozy', 'neighbourhood_group'])['price'].mean())\n",
        "\n",
        "## reshape to wide format so that each borough is row\n",
        "## and one col is the mean price for listings that describe\n",
        "## the place as cozy; other col is mean price for listings\n",
        "## without that word\n",
        "mp_wide = pd.pivot_table(mp, index = ['neighbourhood_group'],\n",
        "                        columns = ['is_cozy'])\n",
        "\n",
        "mp_wide.columns = ['no_mention_cozy', 'mention_cozy']\n",
        "\n",
        "mp_wide"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no_mention_cozy</th>\n",
              "      <th>mention_cozy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neighbourhood_group</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bronx</th>\n",
              "      <td>89.231088</td>\n",
              "      <td>74.214286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brooklyn</th>\n",
              "      <td>128.175441</td>\n",
              "      <td>91.130224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Manhattan</th>\n",
              "      <td>204.109775</td>\n",
              "      <td>129.917140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Queens</th>\n",
              "      <td>102.596682</td>\n",
              "      <td>80.344388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Staten Island</th>\n",
              "      <td>120.650307</td>\n",
              "      <td>74.319149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     no_mention_cozy  mention_cozy\n",
              "neighbourhood_group                               \n",
              "Bronx                      89.231088     74.214286\n",
              "Brooklyn                  128.175441     91.130224\n",
              "Manhattan                 204.109775    129.917140\n",
              "Queens                    102.596682     80.344388\n",
              "Staten Island             120.650307     74.319149"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac5hJRpWxmWz"
      },
      "source": [
        "## Manual approach 2: score based on dictionary of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nfQAnszxmWz",
        "outputId": "075edbed-7e31-4d29-c848-c92dad9064a2"
      },
      "source": [
        "## construct dictionary\n",
        "space_indicators = {'small': ['COZY', 'COMFY', 'LITTLE', 'SMALL'],\n",
        "                   'large': ['SPACIOUS', 'LARGE', 'HUGE', 'GIANT']}\n",
        "\n",
        "\n",
        "## for each listing, find the number of occurrences\n",
        "## of words in each key\n",
        "\n",
        "### first, let's test with one listing\n",
        "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
        "\n",
        "### splitting that string at space and looking at overlap with each key\n",
        "### first, look at overlap with the list containing words for small\n",
        "words_overlap_small = [word for word in practice_listing.split(\" \") if \n",
        "                      word in space_indicators['small']]\n",
        "words_overlap_small\n",
        "\n",
        "### then, look at overlap with the list containing words for large\n",
        "words_overlap_large = [word for word in practice_listing.split(\" \") if \n",
        "                      word in space_indicators['large']]\n",
        "words_overlap_large\n",
        "\n",
        "### could then take length as a fraction of all words\n",
        "len(words_overlap_small)/len(practice_listing.split(\" \"))\n",
        "len(words_overlap_large)/len(practice_listing.split(\" \"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['COZY', 'LITTLE']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dL4Z__dxmW0"
      },
      "source": [
        "## Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhXJkndlxmW0",
        "outputId": "75cc6f8f-f322-4f30-cad5-c016434d1350"
      },
      "source": [
        "## specify example\n",
        "example_for_tag = \"This is a chill apt next to the subway in LES Chinatown\"\n",
        "example_for_tag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a chill apt next to the subway in LES Chinatown'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm7w2t3uxmW0",
        "outputId": "b092db2b-b363-4a2a-8080-dec10dfa4a74"
      },
      "source": [
        "## try part of speech tagging using nltk\n",
        "tokens = word_tokenize(example_for_tag) # Generate list of tokens\n",
        "tokens_pos = pos_tag(tokens) # generate part of speech tags for those tokens\n",
        " \n",
        "## returns a list of tuples\n",
        "## first element in tuple is a word\n",
        "## second element in tuple is the part of speech\n",
        "for one_tok in tokens_pos:\n",
        "    print(one_tok)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('This', 'DT')\n",
            "('is', 'VBZ')\n",
            "('a', 'DT')\n",
            "('chill', 'NN')\n",
            "('apt', 'JJ')\n",
            "('next', 'JJ')\n",
            "('to', 'TO')\n",
            "('the', 'DT')\n",
            "('subway', 'NN')\n",
            "('in', 'IN')\n",
            "('LES', 'NNP')\n",
            "('Chinatown', 'NNP')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAZj2azExmW1",
        "outputId": "3e0fd2ef-871b-4ddd-aeff-98ddd77a6ee9"
      },
      "source": [
        "## use list iteration to extract proper nouns (NNP)\n",
        "## i'm first checking if the second element in the tuple\n",
        "## is equal to NNP\n",
        "## if so, i'm returning the first element in the tuple (the \n",
        "## actual word)\n",
        "all_prop_noun = [one_tok[0] for one_tok in tokens_pos \n",
        "                if one_tok[1] == \"NNP\"]\n",
        "all_prop_noun"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LES', 'Chinatown']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJLMEuVuxmW1"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjH6a1EfxmW1"
      },
      "source": [
        "## modified from: https://twitter.com/dartmouth/status/1387488541844856838\n",
        "\n",
        "## tweet\n",
        "d_tweet = \"\"\"Dependents, partners, and household members of\n",
        "Dartmouth College students, staff, and faculty who are 18 or older are\n",
        "now eligible to sign up for COVID-19 vaccination clinics on May 5 and May 6.\n",
        "The deadline to sign up is 11:59 p.m. on April 29. These are in New Hampshire.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MTlpG4WxmW2",
        "outputId": "0db013ef-44b9-41c1-b7df-fea90c0c15ba"
      },
      "source": [
        "spacy_dtweet = nlp(d_tweet)\n",
        "for one_tok in spacy_dtweet.ents:\n",
        "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entity: Dartmouth College; NER tag: ORG\n",
            "Entity: 18 or older; NER tag: DATE\n",
            "Entity: May 5; NER tag: DATE\n",
            "Entity: May 6; NER tag: DATE\n",
            "Entity: 11:59 p.m.; NER tag: TIME\n",
            "Entity: April 29; NER tag: DATE\n",
            "Entity: New Hampshire; NER tag: GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLwMCua2xmW2"
      },
      "source": [
        "## try a couple variations\n",
        "## eg removing college, NH compared to New Hampshire\n",
        "## capitalize faculty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWGvV8AMxmW2"
      },
      "source": [
        "## Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDtsGjSxmW2"
      },
      "source": [
        "### Using the default scorer on a few example phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isPUQbr4xmW2",
        "outputId": "3ca4d230-1158-4d10-84cb-28e12a08d3e1"
      },
      "source": [
        "## initialize a scorer\n",
        "sent_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "## score one listing\n",
        "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
        "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
        "sentiment_example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwOXIzUixmW3"
      },
      "source": [
        "## adding phrase with word terrible and score\n",
        "practice_listing_2 = \"NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW.\"\n",
        "sentiment_example_2 = sent_obj.polarity_scores(practice_listing_2)\n",
        "\n",
        "## adding phrase about rats; bad but might not be in scoring dictionary\n",
        "practice_listing_3 = \"NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH.\"\n",
        "sentiment_example_3 = sent_obj.polarity_scores(practice_listing_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaJRgJmixmW3",
        "outputId": "7ffac012-5e7f-45a1-d945-1a219a4ce7db"
      },
      "source": [
        "## summarize all 3\n",
        "print(\"String: \" + practice_listing + \" scored as:\\n\" + str(sentiment_example))\n",
        "print(\"String: \" + practice_listing_2 + \" scored as:\\n\" + str(sentiment_example_2))\n",
        "print(\"String: \" + practice_listing_3 + \" scored as:\\n\" + str(sentiment_example_3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "String: NICE AND COZY LITTLE APT AVAILABLE scored as:\n",
            "{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}\n",
            "String: NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW. scored as:\n",
            "{'neg': 0.257, 'neu': 0.531, 'pos': 0.212, 'compound': -0.1513}\n",
            "String: NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH. scored as:\n",
            "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.4215}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u1JWsyhxmW4"
      },
      "source": [
        "### Updating the dictionary with manually-added words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px3TD-DsxmW4",
        "outputId": "d64e8b36-af32-4555-f020-1b0f03aacf0d"
      },
      "source": [
        "## lexicon is a dictionary where the key\n",
        "## is the word\n",
        "## the value is the score (negative = negative)\n",
        "## here, i'm benchmarking the negativity of the\n",
        "## rodents to the negativity of the word aversion\n",
        "sent_obj.lexicon['aversion']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzbulW5LxmW4",
        "outputId": "ad27a834-236c-4b41-c813-2cd4ab2c4dc9"
      },
      "source": [
        "## create a dictionary with \n",
        "## negative scores for pests\n",
        "pest_words = {\n",
        "    'rat': -1.9,\n",
        "    'rats': -1.9,\n",
        "    'mice': -1.9,\n",
        "    'mouse': -1.9,\n",
        "    'roach': -1.9,\n",
        "    'cockroach': -1.9\n",
        "}\n",
        "\n",
        "\n",
        "## initiate new sentiment object\n",
        "## so that we don't alter old one\n",
        "## use.update to add new words\n",
        "new_si = SentimentIntensityAnalyzer()\n",
        "new_si.lexicon.update(pest_words)\n",
        "\n",
        "## try re-scoring the third example\n",
        "## see negative\n",
        "print(\"After lexicon update: \" + practice_listing_3 + \" scored as:\\n\" + \\\n",
        "      str(new_si.polarity_scores(practice_listing_3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After lexicon update: NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH. scored as:\n",
            "{'neg': 0.228, 'neu': 0.551, 'pos': 0.22, 'compound': -0.0258}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}