{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "06_textasdata_partII_topicmodeling_examplecode.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew66882011/qss20_slides_activities/blob/main/activities/06_textasdata_partII_topicmodeling_examplecode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf5CO8kMLlQb"
      },
      "source": [
        "# Notebook for topic modeling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHwJqiBFLlQe"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbX8NacfLlQf"
      },
      "source": [
        "## load packages \n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "## nltk imports\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "## sklearn imports\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## lda \n",
        "from gensim import corpora\n",
        "import gensim\n",
        "\n",
        "## print mult things\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "## random\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M3Bg9AeLlQg"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwHQnFyfLlQh"
      },
      "source": [
        "ab = pd.read_csv(\"../public_data/airbnb_text.zip\")\n",
        "ab.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ZJiITVLlQi"
      },
      "source": [
        "## Preprocess documents\n",
        "\n",
        "In this case, each name/name_upper, or listing title, we're treating as a document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2cUKJreLlQj"
      },
      "source": [
        "### Step 1- load stopwords and augment with our own custom ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPIuPJiFLlQj"
      },
      "source": [
        "list_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "custom_words_toadd = ['apartment', 'new york', 'nyc',\n",
        "                      'bronx', 'brooklyn',\n",
        "                     'manhattan', 'queens', \n",
        "                      'staten island']\n",
        "\n",
        "list_stopwords_new = list_stopwords + custom_words_toadd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LksV-AMLlQk"
      },
      "source": [
        "### Step 2- remove stopwords from lowercase version of corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIfrOiAMLlQl"
      },
      "source": [
        "## convert to lowercase and a list\n",
        "corpus_lower = ab.name.str.lower().to_list()\n",
        "corpus_lower[0:5]\n",
        "\n",
        "## use wordpunct tokenize and filter out with one\n",
        "example_listing = corpus_lower[3]\n",
        "nostop_listing = [word for word in wordpunct_tokenize(example_listing) \n",
        "                          if word not in list_stopwords_new]\n",
        "nostop_listing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvdq59OLLlQm"
      },
      "source": [
        "### Step 3- stem and remove non-alpha\n",
        "\n",
        "Other contexts we may want to leave digits in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU19lX-4LlQm"
      },
      "source": [
        "## initialize stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "## apply to one by iterating\n",
        "## over the tokens in the list\n",
        "example_listing_preprocess = [porter.stem(token) \n",
        "                            for token in nostop_listing \n",
        "                            if token.isalpha() and \n",
        "                            len(token) > 2]\n",
        "\n",
        "example_listing_preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7dUE_I1LlQn"
      },
      "source": [
        "example_listing\n",
        "example_listing_preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPfu-hrHLlQo"
      },
      "source": [
        "# Activity\n",
        "\n",
        "- Embed steps two and three into one or two functions\n",
        "- Apply the function to all the texts in `corpus_lower`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ybj4gz7LlQo"
      },
      "source": [
        "## Create a document-term matrix and do some basic diagnostics (more manual approach)\n",
        "\n",
        "Here we'll create a DTM first using the raw documents; in the activity, you'll create one using the preprocessed docs\n",
        "that you created in the previous activity break"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRvjLcVCLlQp"
      },
      "source": [
        "## function I'm providing\n",
        "def create_dtm(list_of_strings, metadata):\n",
        "    vectorizer = CountVectorizer(lowercase = True)\n",
        "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
        "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names())\n",
        "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
        "    return(dtm_dense_named_withid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oOpl2VfLlQp"
      },
      "source": [
        "## first, filter out na's\n",
        "corpus_lower_nonull = ab.name[~ab.name.isnull()].str.lower()\n",
        "\n",
        "## filter out na's\n",
        "## for shorter runtime, random sampling of 1000\n",
        "## get metadata for those\n",
        "## and also renaming price col since it's likely to be corpus word\n",
        "ab_small = ab.loc[(ab.name.str.lower().isin(corpus_lower_small)) & \n",
        "                  (~ab.name.isnull()),\n",
        "           ['id', 'neighbourhood_group', 'price', 'name']].copy().rename(columns = {'price':\n",
        "            'price_rawdata'}).sample(n = 1000, random_state = 422)\n",
        "\n",
        "ab_small['name_lower'] = ab_small['name'].str.lower()\n",
        "ab_small.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC5s2LAmLlQq"
      },
      "source": [
        "\n",
        "## example application on raw lowercase texts; \n",
        "dtm_nopre = create_dtm(list_of_strings= ab_small.name_lower,\n",
        "                      metadata = ab_small[['id', 'neighbourhood_group', 'price_rawdata']])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1U5XsflLlQr"
      },
      "source": [
        "## first set of rows/cols\n",
        "dtm_nopre.head()\n",
        "\n",
        "## arbitrary later cols\n",
        "dtm_nopre.shape\n",
        "dtm_nopre.iloc[0:5, 480:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uil-JsILlQs"
      },
      "source": [
        "### Can use column sums on that dtm to get basic summary stats of top words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vshoQ53HLlQs"
      },
      "source": [
        "## summing each col\n",
        "top_terms = dtm_nopre[dtm_nopre.columns[4:]].sum(axis = 0)\n",
        "\n",
        "## sorting from most frequent to least frequent\n",
        "top_terms.sort_values(ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIIomthRLlQs"
      },
      "source": [
        "## Use built in functions within gensim to skip some steps and to estimate a topic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s52jOs4LlQt"
      },
      "source": [
        "### Creating the objects to feed the LDA modeling function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaFQ_D4fLlQt"
      },
      "source": [
        "\n",
        "## Step 1: re-tokenize and store in list\n",
        "## here, i'm doing with the raw random sample of text\n",
        "## in activity, you should do with the preprocessed texts\n",
        "text_raw_tokens = [wordpunct_tokenize(one_text) for one_text in \n",
        "                  ab_small.name_lower]\n",
        "\n",
        "text_raw_tokens[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPSmUiZZLlQu"
      },
      "source": [
        "## Step 2: use gensim create dictionary - gets all unique words across documents\n",
        "text_raw_dict = corpora.Dictionary(text_raw_tokens)\n",
        "\n",
        "### explore first few keys and values\n",
        "### see that key is just an arbitrary counter; value is the word itself\n",
        "firstkpairs = {k: text_raw_dict[k] for k in list(text_raw_dict)[:5]}\n",
        "firstkpairs\n",
        "\n",
        "## Step 3: filter out very rare and very common words\n",
        "## here, i'm using the threshold that a word needs to appear in at least\n",
        "## 5% of docs but not more than 95%\n",
        "## this is an integer count of docs so i round\n",
        "lower_bound = round(ab_small.shape[0]*0.05)\n",
        "lower_bound\n",
        "upper_bound = round(ab_small.shape[0]*0.95)\n",
        "upper_bound\n",
        "### apply filtering to dictionary\n",
        "text_raw_dict.filter_extremes(no_below = lower_bound,\n",
        "                             no_above = upper_bound)\n",
        "\n",
        "## Step 4: apply dictionary to TOKENIZED texts\n",
        "## this creates a mapping between each word \n",
        "## in a specific listing and the key in the dictionary\n",
        "## for words that remain in the filtered dictionary\n",
        "## output is a list where len(list) == n documents\n",
        "## and each element in the list is a list of tuples\n",
        "## containing the mappings\n",
        "corpus_fromdict = [text_raw_dict.doc2bow(one_text) \n",
        "                   for one_text in text_raw_tokens]\n",
        "corpus_fromdict[0:5]\n",
        "text_raw_tokens[0:5]\n",
        "\n",
        "### can apply doc2bow(one_text, return_missing = True) to print words\n",
        "### eliminated from the listing bc they're not in filtered dictionary\n",
        "### but feeding that one with missing values to\n",
        "### the lda function can cause errors\n",
        "corpus_fromdict_showmiss = [text_raw_dict.doc2bow(one_text, return_missing = True)\n",
        "                            for one_text in text_raw_tokens]\n",
        "corpus_fromdict_showmiss[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCFp6fj8LlRf"
      },
      "source": [
        "### Estimating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph1xhqG-LlRg"
      },
      "source": [
        "## Step 5: we're finally ready to estimate the model!\n",
        "## full documentation here - https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "## here, we're feed the lda function (1) the corpus we created from the dictionary\n",
        "## (2) a parameter we decide on for the number of topics,\n",
        "## (3) the dictionary itself,\n",
        "## (4) parameter for number of passes through training data\n",
        "## (5) parameter that returns, for each word remaining in dict, the \n",
        "## topic probabilities\n",
        "## see documentation for many other arguments you can vary\n",
        "ldamod = gensim.models.ldamodel.LdaModel(corpus_fromdict, \n",
        "                                         num_topics = 5, id2word=text_raw_dict, \n",
        "                                         passes=6, alpha = 'auto',\n",
        "                                        per_word_topics = True)\n",
        "\n",
        "print(type(ldamod))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiopSDC4LlRh"
      },
      "source": [
        "### Post-model exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKIOjkvrLlRh"
      },
      "source": [
        "## Post-model 1: explore corpus-wide summary of topics\n",
        "### getting the topics and top words; can retrieve diff top words\n",
        "topics = ldamod.print_topics(num_words = 10)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAdWQklNLlRh"
      },
      "source": [
        "    \n",
        "## Post-model 2: explore topics associated with each document\n",
        "### for each item in our original dictionary, get list of topic probabilities\n",
        "l=[ldamod.get_document_topics(item) for item in corpus_fromdict]\n",
        "### print result\n",
        "text_raw_tokens[0:5]\n",
        "l[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijln8W8mLlRi"
      },
      "source": [
        "### Visualizing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5LvAP6fLlRi"
      },
      "source": [
        "## Visualize - may not work on jhub yet\n",
        "import pyLDAvis.gensim as gensimvis\n",
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "lda_display = gensimvis.prepare(ldamod, corpus_fromdict, text_raw_dict)\n",
        "pyLDAvis.display(lda_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqAY65m6LlRi"
      },
      "source": [
        "# Activity\n",
        "\n",
        "- Preprocess the texts\n",
        "- Repeat the preprocessing steps and running of the topic model with preprocessed texts (can also play around with other parameters like n_topics)- what seems to produce useful topics?\n",
        "\n",
        "\n",
        "If you get stuck on the preprocessing part, you can use below function and I show example of how to apply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du6Fn-9_LlRj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7pmMrHsLlRj"
      },
      "source": [
        "def processtext(row, colname):\n",
        "    \n",
        "    string_of_col = str(row[colname])\n",
        "    try:\n",
        "        processed_string = \" \".join([porter.stem(i.lower()) for i in wordpunct_tokenize(string_of_col) if \n",
        "                        i.lower().isalpha() and len(i) >=3])  \n",
        "        return(processed_string)\n",
        "    except:\n",
        "        processed_string = \"\" # to handle data errors where not actually text\n",
        "        return(processed_string)\n",
        "\n",
        "# ab_small['text_preprocess'] = ab_small.apply(processtext,\n",
        "#                             axis = 1,\n",
        "#                             args = [\"name_lower\"])\n",
        "# ab_small.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}