{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "06_textasdata_partI_textmining_examplecode.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew66882011/qss20_slides_activities/blob/main/activities/06_textasdata_partI_textmining_examplecode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXTBmPRxmWt"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPIbghTAx0JV"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQM4zTQyxmWv"
      },
      "source": [
        "## load packages \n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "## nltk imports\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "## uncomment and download if this is your first \n",
        "## time running \n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "## sentiment analysis\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "## specify to print all output in a call\n",
        "## and not just first\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxmOQMMtxmWw"
      },
      "source": [
        "## spacy (still being installed on jhub)\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atoyFvW_xmWw"
      },
      "source": [
        "# Load data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqPu86hmyA76"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/andrew66882011/qss20_slides_activities.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Cuw2n9y4VN"
      },
      "source": [
        "!unzip ./public_data/airbnb_text.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSutE24VxmWw"
      },
      "source": [
        "## if working from within the repo, can use this relative path\n",
        "path_todata = \"./airbnb_text.csv\"\n",
        "\n",
        "## load data\n",
        "ab = pd.read_csv(path_todata)\n",
        "ab.head()\n",
        "ab.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RmNTCoxmWx"
      },
      "source": [
        "# Text mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK7DnGaTxmWy"
      },
      "source": [
        "## Manual approach 1: look for a single word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1FtdHrixmWy"
      },
      "source": [
        "## using the `name_upper` var, look at where reviews mention cozy\n",
        "ab['is_cozy'] = np.where(ab.name_upper.str.contains(\"COZY\"), True, False)\n",
        "\n",
        "## find the mean price by neighborhood and whether cozy\n",
        "mp = pd.DataFrame(ab.groupby(['is_cozy', 'neighbourhood_group'])['price'].mean())\n",
        "\n",
        "## reshape to wide format so that each borough is row\n",
        "## and one col is the mean price for listings that describe\n",
        "## the place as cozy; other col is mean price for listings\n",
        "## without that word\n",
        "mp_wide = pd.pivot_table(mp, index = ['neighbourhood_group'],\n",
        "                        columns = ['is_cozy'])\n",
        "\n",
        "mp_wide.columns = ['no_mention_cozy', 'mention_cozy']\n",
        "\n",
        "mp_wide"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac5hJRpWxmWz"
      },
      "source": [
        "## Manual approach 2: score based on dictionary of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nfQAnszxmWz"
      },
      "source": [
        "## construct dictionary\n",
        "space_indicators = {'small': ['COZY', 'COMFY', 'LITTLE', 'SMALL'],\n",
        "                   'large': ['SPACIOUS', 'LARGE', 'HUGE', 'GIANT']}\n",
        "\n",
        "\n",
        "## for each listing, find the number of occurrences\n",
        "## of words in each key\n",
        "\n",
        "### first, let's test with one listing\n",
        "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
        "\n",
        "### splitting that string at space and looking at overlap with each key\n",
        "### first, look at overlap with the list containing words for small\n",
        "words_overlap_small = [word for word in practice_listing.split(\" \") if \n",
        "                      word in space_indicators['small']]\n",
        "words_overlap_small\n",
        "\n",
        "### then, look at overlap with the list containing words for large\n",
        "words_overlap_large = [word for word in practice_listing.split(\" \") if \n",
        "                      word in space_indicators['large']]\n",
        "words_overlap_large\n",
        "\n",
        "### could then take length as a fraction of all words\n",
        "len(words_overlap_small)/len(practice_listing.split(\" \"))\n",
        "len(words_overlap_large)/len(practice_listing.split(\" \"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dL4Z__dxmW0"
      },
      "source": [
        "## Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vss2Co7B0Udu"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhXJkndlxmW0"
      },
      "source": [
        "## specify example\n",
        "example_for_tag = \"This is a chill apt next to the subway in LES Chinatown\"\n",
        "example_for_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er6FEQvM0xQp"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm7w2t3uxmW0"
      },
      "source": [
        "## try part of speech tagging using nltk\n",
        "tokens = word_tokenize(example_for_tag) # Generate list of tokens\n",
        "tokens_pos = pos_tag(tokens) # generate part of speech tags for those tokens\n",
        " \n",
        "## returns a list of tuples\n",
        "## first element in tuple is a word\n",
        "## second element in tuple is the part of speech\n",
        "for one_tok in tokens_pos:\n",
        "    print(one_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAZj2azExmW1"
      },
      "source": [
        "## use list iteration to extract proper nouns (NNP)\n",
        "## i'm first checking if the second element in the tuple\n",
        "## is equal to NNP\n",
        "## if so, i'm returning the first element in the tuple (the \n",
        "## actual word)\n",
        "all_prop_noun = [one_tok[0] for one_tok in tokens_pos \n",
        "                if one_tok[1] == \"NNP\"]\n",
        "all_prop_noun"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJLMEuVuxmW1"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjH6a1EfxmW1"
      },
      "source": [
        "## modified from: https://twitter.com/dartmouth/status/1387488541844856838\n",
        "\n",
        "## tweet\n",
        "d_tweet = \"\"\"Dependents, partners, and household members of\n",
        "Dartmouth College students, staff, and faculty who are 18 or older are\n",
        "now eligible to sign up for COVID-19 vaccination clinics on May 5 and May 6.\n",
        "The deadline to sign up is 11:59 p.m. on April 29. These are in New Hampshire.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MTlpG4WxmW2"
      },
      "source": [
        "spacy_dtweet = nlp(d_tweet)\n",
        "for one_tok in spacy_dtweet.ents:\n",
        "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLwMCua2xmW2"
      },
      "source": [
        "## try a couple variations\n",
        "## eg removing college, NH compared to New Hampshire\n",
        "## capitalize faculty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWGvV8AMxmW2"
      },
      "source": [
        "## Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDtsGjSxmW2"
      },
      "source": [
        "### Using the default scorer on a few example phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isPUQbr4xmW2"
      },
      "source": [
        "## initialize a scorer\n",
        "sent_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "## score one listing\n",
        "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
        "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
        "sentiment_example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwOXIzUixmW3"
      },
      "source": [
        "## adding phrase with word terrible and score\n",
        "practice_listing_2 = \"NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW.\"\n",
        "sentiment_example_2 = sent_obj.polarity_scores(practice_listing_2)\n",
        "\n",
        "## adding phrase about rats; bad but might not be in scoring dictionary\n",
        "practice_listing_3 = \"NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH.\"\n",
        "sentiment_example_3 = sent_obj.polarity_scores(practice_listing_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaJRgJmixmW3"
      },
      "source": [
        "## summarize all 3\n",
        "print(\"String: \" + practice_listing + \" scored as:\\n\" + str(sentiment_example))\n",
        "print(\"String: \" + practice_listing_2 + \" scored as:\\n\" + str(sentiment_example_2))\n",
        "print(\"String: \" + practice_listing_3 + \" scored as:\\n\" + str(sentiment_example_3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u1JWsyhxmW4"
      },
      "source": [
        "### Updating the dictionary with manually-added words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px3TD-DsxmW4"
      },
      "source": [
        "## lexicon is a dictionary where the key\n",
        "## is the word\n",
        "## the value is the score (negative = negative)\n",
        "## here, i'm benchmarking the negativity of the\n",
        "## rodents to the negativity of the word aversion\n",
        "sent_obj.lexicon['aversion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzbulW5LxmW4"
      },
      "source": [
        "## create a dictionary with \n",
        "## negative scores for pests\n",
        "pest_words = {\n",
        "    'rat': -1.9,\n",
        "    'rats': -1.9,\n",
        "    'mice': -1.9,\n",
        "    'mouse': -1.9,\n",
        "    'roach': -1.9,\n",
        "    'cockroach': -1.9\n",
        "}\n",
        "\n",
        "\n",
        "## initiate new sentiment object\n",
        "## so that we don't alter old one\n",
        "## use.update to add new words\n",
        "new_si = SentimentIntensityAnalyzer()\n",
        "new_si.lexicon.update(pest_words)\n",
        "\n",
        "## try re-scoring the third example\n",
        "## see negative\n",
        "print(\"After lexicon update: \" + practice_listing_3 + \" scored as:\\n\" + \\\n",
        "      str(new_si.polarity_scores(practice_listing_3)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}