{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "05_merging_session2_codeexample.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew66882011/qss20_slides_activities/blob/main/activities/05_merging_session2_codeexample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e_kTue9Jkq1"
      },
      "source": [
        "# Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfZjIYSEJkq4"
      },
      "source": [
        "import pandas as pd\n",
        "import re \n",
        "import numpy as np\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "\n",
        "## a couple recordlinkage packages\n",
        "import fuzzywuzzy\n",
        "import recordlinkage\n",
        "\n",
        "## repeated printouts\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP9Ue-ICJkq5"
      },
      "source": [
        "\n",
        "## nltk for string distance\n",
        "import nltk\n",
        "\n",
        "## jarowinkler\n",
        "from pyjarowinkler import distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFVYyNEpJkq6"
      },
      "source": [
        "# Load and view dataset 1: tax certificates for San Diego businesses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5gtvt-9Jkq6"
      },
      "source": [
        "## general link: https://data.sandiego.gov/datasets/business-listings/\n",
        "\n",
        "## active tax certificates\n",
        "sd = pd.read_csv(\"https://seshat.datasd.org/ttcs/sd_businesses_active_datasd.csv\")\n",
        "sd.head()\n",
        "\n",
        "## PPP loans for CA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfxuzGp5Jkq7"
      },
      "source": [
        "## Load and view dataset 2: PPP loans > 150k\n",
        "\n",
        "General link: https://data.sba.gov/dataset/ppp-foia/resource/3d28c417-5170-4f1f-be31-b0c7b0182501 \n",
        "        \n",
        "\n",
        "For a real application, we'd want to programmatically load and rowbind the different < 150k sheets. For this exercise,\n",
        "we'll just look at the larger loans (>150k) and subset to california"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zMNabIAJkq8"
      },
      "source": [
        "ppp = pd.read_csv(\"https://data.sba.gov/dataset/8aa276e2-6cab-4f86-aca4-a7dde42adf24/resource/6b62a44b-69ec-436a-9b95-0ea550475543/download/public_150k_plus.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgocNUXxJkq8"
      },
      "source": [
        "\n",
        "## look at address fields to see whether state is relatively complete\n",
        "## see that state is only missing about 14 so we (1) subset to CA and \n",
        "## (2) subset to State == CA and also\n",
        "## zips that overlap with ones in the SD tax certificate data\n",
        "## just using raw-zip but to be more careful, if doing for real,\n",
        "## we'd want to standardize to either 6 dig zip or 10-dig zip\n",
        "ppp.BorrowerState.value_counts(dropna = False)\n",
        "ppp = ppp[(ppp.BorrowerState == \"CA\") &\n",
        "                  (ppp.BorrowerZip.isin(sd.address_zip))].copy()\n",
        "ppp.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUuA1A6TJkq-"
      },
      "source": [
        "## Step 1 - what are the possible join fields between the two:\n",
        "\n",
        "San Diego tax certicate:\n",
        "\n",
        "- Business-level fields:\n",
        "    - Owner name\n",
        "    - Business name (dba_name)\n",
        "    \n",
        "- Sector-level fields:\n",
        "    - naics_sector \n",
        "    - naics_code\n",
        "    - naics_description\n",
        "    \n",
        "- Geographic fields:\n",
        "    - City and state \n",
        "    - Zip \n",
        "    - Bid (business improvement district)\n",
        "    - Council district\n",
        "    - Address\n",
        "    \n",
        "PPP loans:\n",
        "\n",
        "- Business-level:\n",
        "    - BorrowerName\n",
        "    - Borrower Address\n",
        "    - BorrowerCity\n",
        "    - BorrowerState\n",
        "    - BorrowerZip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hhyN8IiJkq-"
      },
      "source": [
        "## Step 2-- build our matching approach using some manual examples\n",
        "\n",
        "Examples of two PPP loan recipients:\n",
        "\n",
        "- THE KLEINFELDER GROUP, INC.\n",
        "- DURAN FREIGHT CORPORATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyx60wSpJkq_"
      },
      "source": [
        "klein_patt = r\".*(\\s+)?KLEINFELDER\\s+.*\"\n",
        "klein_possible = [biz for biz in sd.dba_name\n",
        "                 if re.match(klein_patt, biz) is not None]\n",
        "klein_possible"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2G0pjuUJkq_"
      },
      "source": [
        "duran_patt = r\".*(\\s+)?DURAN\\s+.*\"\n",
        "duran_possible = [biz for biz in sd.dba_name\n",
        "                 if re.match(duran_patt, biz) is not None]\n",
        "duran_possible"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijHSLCuwJkq_"
      },
      "source": [
        "### Investigate fields that could help weed out false matches for the first business"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R9ptffBJkrA"
      },
      "source": [
        "sd.columns\n",
        "ppp.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZUbEiDBJkrA"
      },
      "source": [
        "## defining helpful fields w/in each df for adjudicating matches\n",
        "ppp_helpfulfields = [\"BorrowerName\", \"BorrowerAddress\", \"BorrowerCity\", \n",
        "                    \"BorrowerZip\", \"FranchiseName\", \"NAICSCode\", \"ProjectZip\"]\n",
        "sd_helpfulfields = [\"dba_name\", \"naics_code\", \"naics_sector\",\n",
        "                    \"address_no\", \"address_pd\", \"address_road\",\n",
        "                    \"address_sfx\", \"address_city\", \"address_zip\", \"date_cert_effective\",\n",
        "                   \"date_cert_expiration\", \"business_owner_name\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMwyUIKEJkrB"
      },
      "source": [
        "print(ppp.loc[ppp.BorrowerName == \"THE KLEINFELDER GROUP, INC.\",\n",
        "                  ppp_helpfulfields])\n",
        "\n",
        "\n",
        "print(sd.loc[sd.dba_name.isin(klein_possible),\n",
        "                       sd_helpfulfields])\n",
        "\n",
        "## see that likely either kleinfelder construction services\n",
        "## or kleinfelder inc (could match to both); possible slight \n",
        "## pref for matching to kleinfelder inc owner name "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opwxiXhAJkrB"
      },
      "source": [
        "### Preview of activity step 1: clean addresses in each of the datasets\n",
        "\n",
        "Previous example shows us address can help adjudicate b/t matches.\n",
        "\n",
        "When we break into groups, you'll\n",
        "    \n",
        "- Create a new zip code col that's just the first 6 digits\n",
        "- Paste together the address_no, address_pd, address_road, address_sfx fields in the SD active biz to create a field similar to BorrowerAddress in the PPP loan data (pay attention to capitalization; might be easier to capitalize in each)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zafUs7d5JkrB"
      },
      "source": [
        "# Constructing our own matching function\n",
        "\n",
        "The package we'll review makes matching easier by putting a lot of the hard stuff under the hood\n",
        "\n",
        "But it's good to know what's going on under that hood.\n",
        "\n",
        "Here, using the example of THE KLEINFELDER GROUP, INC., we'll look within the tentative matches + a random other sample of the SD business data to construct match points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8QcOIWfJkrC"
      },
      "source": [
        "## Step 0: pool of sd businesses to look in\n",
        "\n",
        "Normally we'd look in full set but this helps with runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL1w-P31JkrC"
      },
      "source": [
        "sd = pd.concat([sd[sd.dba_name.isin(klein_possible)].copy(),\n",
        "                         sd[~sd.dba_name.isin(klein_possible)].sample(n = 15, \n",
        "                        random_state = 922).copy(),\n",
        "                          sd[sd.dba_name == \"DURAN FREIGHT CORPORATION\"]])\n",
        "\n",
        "sd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tDMBbfeJkrD"
      },
      "source": [
        "## Step 1: find string similarity between (1) our focal PPP business (Kleinfelder) and (2) the businesses in the SD pool\n",
        "\n",
        "Here, we're using Jaccard distance --- common one in addition to that is Jaro Winkler string similarity\n",
        "\n",
        "Some options here: https://python.gotrained.com/nltk-edit-distance-jaccard-distance/\n",
        "\n",
        "Can also use fuzzywuzzy installed on jhub- discussion here: https://towardsdatascience.com/fuzzy-string-matching-in-python-68f240d910fe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-WDWyKFJkrD"
      },
      "source": [
        "## first, let's process the biz name\n",
        "## and remove everything that's not (^)\n",
        "## words or spaces and also remove the\n",
        "focal_ppp_raw = \"THE KLEINFELDER GROUP, INC.\"\n",
        "focal_ppp_cleaner = re.sub(\"THE\\s\", \n",
        "                           \"\", \n",
        "                    re.sub(r\"[^\\w\\s]\", \"\", focal_ppp_raw))\n",
        "focal_ppp_cleaner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FoB2VU0JkrE"
      },
      "source": [
        "### look at a few different distance metrics\n",
        "sd['dist_focal_edit'] = [nltk.edit_distance(focal_ppp_cleaner, other_name)\n",
        "                     for other_name in sd.dba_name]\n",
        "\n",
        "sd[['dba_name', 'dist_focal_edit']].sort_values(by = 'dist_focal_edit')\n",
        "\n",
        "sd['dist_focal_jacc'] = [nltk.jaccard_distance(set(focal_ppp_cleaner), set(other_name))\n",
        "                     for other_name in sd.dba_name]\n",
        "\n",
        "sd[['dba_name', 'dist_focal_jacc']].sort_values(by = 'dist_focal_jacc')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BYSJPa6JkrE"
      },
      "source": [
        "## jaro is similarity score so 1 - that\n",
        "sd['dist_focal_jaro'] = [1-distance.get_jaro_distance(focal_ppp_cleaner, other_name,\n",
        "                                                              winkler = True, scaling = 0.1)\n",
        "                     for other_name in sd.dba_name]\n",
        "\n",
        "sd[['dba_name', 'dist_focal_jaro']].sort_values(by = 'dist_focal_jaro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o04Si9iJkrF"
      },
      "source": [
        "### Step 2-- rule out potential matches with different zip codes\n",
        "\n",
        "\"Blocking\" on 6-digit zip code, or requiring an exact match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tebd4VSJkrF"
      },
      "source": [
        "## first, we clean up the SD zip codes to only be 6 dig since we know our focal ppp biz\n",
        "## has a 6-dig zip code\n",
        "sd['zip_6dig'] = sd.address_zip.str.replace(\"\\-.*\", \"\", regex = True)\n",
        "\n",
        "## get the zip- using iloc since we just want it as a string\n",
        "## rather than series\n",
        "focal_ppp_zip = ppp.BorrowerZip[ppp.BorrowerName == \"THE KLEINFELDER GROUP, INC.\"].iloc[0]\n",
        "focal_ppp_zip\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiQ5f7cxJkrF"
      },
      "source": [
        "## create true false if same as focal biz\n",
        "sd['is_match_zip'] = np.where(sd.zip_6dig == focal_ppp_zip,\n",
        "                                        True, False)\n",
        "\n",
        "sd.loc[sd.is_match_zip,\n",
        "             sd_helpfulfields]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Huxfq1hJkrG"
      },
      "source": [
        "### Step 3: construct some match score\n",
        "\n",
        "Record linkage methods have different ways for aggregating across fields\n",
        "\n",
        "Here, we're going with a simple one of:\n",
        "\n",
        "- Need to match the zip code of the focal Kleinfelder group directly\n",
        "- Within those, find the average of the jarowinkler and jaccard string distance measures (we're excluding edit distance from that avg since on diff scale)\n",
        "\n",
        "Whichever has the lowest average of two we consider the best match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbc1LN3_JkrG"
      },
      "source": [
        "string_dist_fields = [col for col in sd.columns if \"dist_\" in col and \n",
        "                     \"edit\" not in col]\n",
        "string_dist_fields\n",
        "mean_distances = sd[string_dist_fields].mean(axis = 1)\n",
        "\n",
        "mean_distances[0:5]\n",
        "\n",
        "sd['mean_string_dist'] = mean_distances\n",
        "\n",
        "sd.loc[sd.is_match_zip,\n",
        "                 sd_helpfulfields + ['mean_string_dist']].sort_values(by = \"mean_string_dist\")\n",
        "\n",
        "## would go with kleinfelder inc and maybe also\n",
        "## the construction services"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AL39PYlJkrG"
      },
      "source": [
        "# That was a lot of steps. How can we use a package to automate a bit?\n",
        "\n",
        "Google \"fuzzy matching\" or \"probablistic record linkage\" packages in python\n",
        "\n",
        "Here, we'll focus on \n",
        "\n",
        "- recordlinkage. Documentation: https://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opHNoNc3JkrH"
      },
      "source": [
        "## Step 1. Define dataframes to match\n",
        "\n",
        "Here, we'll use two dataframes:\n",
        "\n",
        "- The sd_lookin dataframe we've been working with \n",
        "- A ppp dataframe with (1) our focal business, (2) a small random sample of others, (3) the biz we know has an exact match"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkRznRNxJkrH"
      },
      "source": [
        "## subset of ppp\n",
        "## to help with runtime\n",
        "ppp = pd.concat([ppp[ppp.BorrowerName == focal_ppp_raw].copy(),\n",
        "                       ppp[ppp.BorrowerName != focal_ppp_raw].sample(n = 10, random_state = 42),\n",
        "                       ppp[ppp.BorrowerName == \"DURAN FREIGHT CORPORATION\"]])\n",
        "\n",
        "\n",
        "## clean name similarly to how we did before\n",
        "ppp['bname_clean'] = [re.sub(r\"[^\\w\\s]\", \"\", one_n) for one_n in ppp.BorrowerName]\n",
        "ppp[['BorrowerName', 'bname_clean']].head()\n",
        "\n",
        "## clean zip so that 6 digits\n",
        "ppp['zip_4match'] = ppp.BorrowerZip.astype(str).str.replace(\"\\-.*\", \"\", regex = True)\n",
        "ppp[['BorrowerZip', 'zip_4match']].head()\n",
        "\n",
        "## in exercise, you'll clean address and naics codes \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTmQVkO1JkrH"
      },
      "source": [
        "## Step 2: for ease of use, standardize colnames for the fields we'll use\n",
        "\n",
        "In this practice exercise, we'll use:\n",
        "\n",
        "- Fuzzy match on business name\n",
        "- Exact match on 6-digit zip code\n",
        "\n",
        "We only need to standardize the name of the exact match field, but are here just standardizing all for ease of use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQnsFiEzJkrH"
      },
      "source": [
        "## define rename dictionary for sd_biz and rename saving to new (just for convenience to not reload if we want to\n",
        "## change earlier step)\n",
        "newcols_sd = {'dba_name': 'bizname_4match',\n",
        "           'zip_6dig': 'zip_4match'}\n",
        "\n",
        "sd = sd.rename(columns = newcols_sd, inplace = False)\n",
        "\n",
        "sd[[col for col in sd.columns if \"4match\" in col]].head()\n",
        "\n",
        "\n",
        "## same for ppp data\n",
        "newcols_ppp = {'bname_clean': 'bizname_4match'}\n",
        "\n",
        "ppp = ppp.rename(columns = newcols_ppp, inplace = False)\n",
        "\n",
        "ppp[[col for col in ppp.columns if \"4match\" in col]].head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES0G4YnAJkrI"
      },
      "source": [
        "## Step 3: initialize the match object and tell it if anything to \"block on\" or exact match\n",
        "\n",
        "Here, we're blocking on zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58R1BzStJkrI"
      },
      "source": [
        "## initialize indexer\n",
        "my_recordmatcher = recordlinkage.Index()\n",
        "print(type(my_recordmatcher))\n",
        "\n",
        "## tell it what to block on (skip if not blocking on anything)\n",
        "my_recordmatcher.block(\"zip_4match\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSzYXLWrJkrI"
      },
      "source": [
        "## Step 4: create candidate links based on that blocking variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT71KbDuJkrJ"
      },
      "source": [
        "## then, feed the record matcher the two datasets (must have that blocking variable)\n",
        "## this will create candidate_links that are exact matches on those\n",
        "candidate_links_zip = my_recordmatcher.index(sd, ppp)\n",
        "candidate_links_zip\n",
        "\n",
        "print(type(candidate_links_zip))\n",
        "\n",
        "## see that it's a list of tuples and first element in tuple is index\n",
        "## of first df we feed it; second is index in second df we feed it\n",
        "\n",
        "## print example of links\n",
        "sd.loc[sd.index == 31421,\n",
        "         [col for col in sd.columns if \"4match\" in col]]\n",
        "ppp.loc[ppp.index.isin([80338, 88795]),\n",
        "        [col for col in ppp.columns if \"4match\" in col]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPkv4DdfJkrJ"
      },
      "source": [
        "## Step 5- initialize Compare and define fuzzy fields and threshold for each\n",
        "\n",
        "Note in documentation about diff string compare methods:\n",
        "This class is used to compare string values. The implemented algorithms are: ‘jaro’,’jarowinkler’, ‘levenshtein’, ‘damerau_levenshtein’, ‘qgram’ or ‘cosine’. In case of agreement, the similarity is 1 and in case of complete disagreement it is 0. The Python Record Linkage Toolkit uses the jellyfish package for the Jaro, Jaro-Winkler, Levenshtein and Damerau- Levenshtein algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CE2JJybJkrJ"
      },
      "source": [
        "compare = recordlinkage.Compare()\n",
        "\n",
        "thres_bizname = 0.7\n",
        "compare.string('bizname_4match', 'bizname_4match', method='jaro', threshold=thres_bizname)\n",
        "\n",
        "print(type(compare))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJNaFj42JkrK"
      },
      "source": [
        "## Step 6- using the compare Class and the candidate links, compute comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpVJfMhHJkrK"
      },
      "source": [
        "compare_vectors = compare.compute(candidate_links_zip, sd, ppp)\n",
        "print(type(compare_vectors))\n",
        "\n",
        "compare_vectors\n",
        "\n",
        "## returns result from comparing each pair of records - so we see that with the 2497\n",
        "## example above (kleinfield construction with naics 54161), \n",
        "## which has candidate pairs of (1) Kneinfelder group naics code 541330 (index 34514)\n",
        "## and (2) globe haru naics code 722511 (index 112928), there seems to be a match on name\n",
        "## with the first in the pair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPbwrUR9JkrK"
      },
      "source": [
        "## Step 7. decide what counts as a true match\n",
        "\n",
        "Three general approaches:\n",
        "\n",
        "- Threshold based: look at the raw scores and determine what scores are above a threshold\n",
        "- Unsupervised: something that clusters the pairs into \"likely match\" or \"likely not match\" but where we're not feeding it \"labels\" corresponding to true matches\n",
        "- Supervised: we have some gold-standard label dataset that has an indicator for whether records are true matches; we train a model on those true matches and generalize to new cases\n",
        "\n",
        "See here for many classifiers: https://recordlinkage.readthedocs.io/en/latest/ref-classifiers.html\n",
        "\n",
        "Here, we're using unsupervised and k-means clustering algorithm\n",
        "\n",
        "Other option is an EM-based classifier initialized as follows, but not enough data here to fit:\n",
        "ecm = recordlinkage.ECMClassifier()\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGq7mP28JkrL"
      },
      "source": [
        "## initialize classifier\n",
        "kmeans = recordlinkage.KMeansClassifier()\n",
        "kmeans_results = kmeans.fit_predict(compare_vectors)\n",
        "print(type(kmeans_results))\n",
        "kmeans_results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrIL71O7JkrL"
      },
      "source": [
        "## Step 8- extract pairs using indices and summarize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv_Pqc97JkrL"
      },
      "source": [
        "## since sd was our left hand side data, they're \n",
        "## the first index in the tuple- extract\n",
        "indices_sd = [x[0] for x in kmeans_results]\n",
        "\n",
        "## since ppp loans were our right hand side data, they're\n",
        "## the second index in the tuple - extract\n",
        "indices_ppp = [x[1] for x in kmeans_results]\n",
        "\n",
        "## create dataframe\n",
        "df_matchpairs = pd.DataFrame({'sd_indices': indices_sd,\n",
        "                'ppp_indices': indices_ppp})\n",
        "\n",
        "df_matchpairs\n",
        "\n",
        "## add indices as col to orig data\n",
        "sd['index_4merge'] = sd.index\n",
        "ppp['index_4merge'] = ppp.index\n",
        "\n",
        "## then, join matches\n",
        "\n",
        "### first, i'm joining the sd info\n",
        "df_matchpairs_wsd = pd.merge(df_matchpairs,\n",
        "                            sd[['index_4merge', 'bizname_4match',\n",
        "                                      'zip_4match']],\n",
        "                            how = \"left\",\n",
        "                            left_on = \"sd_indices\",\n",
        "                            right_on = \"index_4merge\")\n",
        "\n",
        "df_matchpairs_wsd\n",
        "\n",
        "## then, i'm joining the ppp info and adding a suffix to distinguish the vars\n",
        "df_matchpairs_wboth = pd.merge(df_matchpairs_wsd,\n",
        "                              ppp[['index_4merge', 'bizname_4match',\n",
        "                                         'zip_4match']],\n",
        "                              how = \"left\",\n",
        "                              left_on = \"ppp_indices\",\n",
        "                              right_on = \"index_4merge\",\n",
        "                              suffixes= [\"_sd_tax\", \"_ppp\"])\n",
        "\n",
        "df_matchpairs_wboth\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}