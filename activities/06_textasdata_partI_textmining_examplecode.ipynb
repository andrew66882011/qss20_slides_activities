{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "06_textasdata_partI_textmining_examplecode.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew66882011/qss20_slides_activities/blob/main/activities/06_textasdata_partI_textmining_examplecode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXTBmPRxmWt"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPIbghTAx0JV",
        "outputId": "f0034ba1-48c5-47f2-ba8d-214fa87c9c07"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQM4zTQyxmWv"
      },
      "source": [
        "## load packages \n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "## nltk imports\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "## uncomment and download if this is your first \n",
        "## time running \n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "## sentiment analysis\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "## specify to print all output in a call\n",
        "## and not just first\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxmOQMMtxmWw"
      },
      "source": [
        "## spacy (still being installed on jhub)\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atoyFvW_xmWw"
      },
      "source": [
        "# Load data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqPu86hmyA76",
        "outputId": "2d1d1725-dd84-4840-d344-eb915ef080ed"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/andrew66882011/qss20_slides_activities.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 332, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 332 (delta 110), reused 22 (delta 22), pack-reused 207\u001b[K\n",
            "Receiving objects: 100% (332/332), 61.50 MiB | 20.54 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/cloned-repo\n",
            "activities  problemsets  public_data  README.md  slides\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Cuw2n9y4VN",
        "outputId": "f6aa4c71-69fc-41ec-c30d-d21826a4e0aa"
      },
      "source": [
        "!unzip ./public_data/airbnb_text.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./public_data/airbnb_text.zip\n",
            "  inflating: airbnb_text.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "aSutE24VxmWw",
        "outputId": "7f756fcd-8438-4d92-aa8b-4dc4309935d5"
      },
      "source": [
        "## if working from within the repo, can use this relative path\n",
        "path_todata = \"./airbnb_text.csv\"\n",
        "\n",
        "## load data\n",
        "ab = pd.read_csv(path_todata)\n",
        "ab.head()\n",
        "ab.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>name_upper</th>\n",
              "      <th>neighbourhood_group</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2539</td>\n",
              "      <td>Clean &amp; quiet apt home by the park</td>\n",
              "      <td>CLEAN &amp; QUIET APT HOME BY THE PARK</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2595</td>\n",
              "      <td>Skylit Midtown Castle</td>\n",
              "      <td>SKYLIT MIDTOWN CASTLE</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3647</td>\n",
              "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
              "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3831</td>\n",
              "      <td>Cozy Entire Floor of Brownstone</td>\n",
              "      <td>COZY ENTIRE FLOOR OF BROWNSTONE</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5022</td>\n",
              "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
              "      <td>ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK</td>\n",
              "      <td>Manhattan</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  ... price\n",
              "0  2539  ...   149\n",
              "1  2595  ...   225\n",
              "2  3647  ...   150\n",
              "3  3831  ...    89\n",
              "4  5022  ...    80\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48895 entries, 0 to 48894\n",
            "Data columns (total 5 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   id                   48895 non-null  int64 \n",
            " 1   name                 48879 non-null  object\n",
            " 2   name_upper           48879 non-null  object\n",
            " 3   neighbourhood_group  48895 non-null  object\n",
            " 4   price                48895 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 1.9+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RmNTCoxmWx"
      },
      "source": [
        "# Text mining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK7DnGaTxmWy"
      },
      "source": [
        "## Manual approach 1: look for a single word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "u1FtdHrixmWy",
        "outputId": "c627abd2-826b-4c0c-93cf-2584a15192ee"
      },
      "source": [
        "## using the `name_upper` var, look at where reviews mention cozy\n",
        "ab['is_cozy'] = np.where(ab.name_upper.str.contains(\"COZY\"), True, False)\n",
        "\n",
        "## find the mean price by neighborhood and whether cozy\n",
        "mp = pd.DataFrame(ab.groupby(['is_cozy', 'neighbourhood_group'])['price'].mean())\n",
        "\n",
        "## reshape to wide format so that each borough is row\n",
        "## and one col is the mean price for listings that describe\n",
        "## the place as cozy; other col is mean price for listings\n",
        "## without that word\n",
        "mp_wide = pd.pivot_table(mp, index = ['neighbourhood_group'],\n",
        "                        columns = ['is_cozy'])\n",
        "\n",
        "mp_wide.columns = ['no_mention_cozy', 'mention_cozy']\n",
        "\n",
        "mp_wide"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no_mention_cozy</th>\n",
              "      <th>mention_cozy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neighbourhood_group</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bronx</th>\n",
              "      <td>89.231088</td>\n",
              "      <td>74.214286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Brooklyn</th>\n",
              "      <td>128.175441</td>\n",
              "      <td>91.130224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Manhattan</th>\n",
              "      <td>204.109775</td>\n",
              "      <td>129.917140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Queens</th>\n",
              "      <td>102.596682</td>\n",
              "      <td>80.344388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Staten Island</th>\n",
              "      <td>120.650307</td>\n",
              "      <td>74.319149</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     no_mention_cozy  mention_cozy\n",
              "neighbourhood_group                               \n",
              "Bronx                      89.231088     74.214286\n",
              "Brooklyn                  128.175441     91.130224\n",
              "Manhattan                 204.109775    129.917140\n",
              "Queens                    102.596682     80.344388\n",
              "Staten Island             120.650307     74.319149"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac5hJRpWxmWz"
      },
      "source": [
        "## Manual approach 2: score based on dictionary of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nfQAnszxmWz",
        "outputId": "2d451d7f-4c5d-4faa-9462-98142a8d77e7"
      },
      "source": [
        "## construct dictionary\n",
        "space_indicators = {'small': ['COZY', 'COMFY', 'LITTLE', 'SMALL'],\n",
        "                   'large': ['SPACIOUS', 'LARGE', 'HUGE', 'GIANT']}\n",
        "\n",
        "\n",
        "## for each listing, find the number of occurrences\n",
        "## of words in each key\n",
        "\n",
        "### first, let's test with one listing\n",
        "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
        "\n",
        "### splitting that string at space and looking at overlap with each key\n",
        "### first, look at overlap with the list containing words for small\n",
        "words_overlap_small = [word for word in practice_listing.split(\" \") if \n",
        "                      word in space_indicators['small']]\n",
        "words_overlap_small\n",
        "\n",
        "### then, look at overlap with the list containing words for large\n",
        "words_overlap_large = [word for word in practice_listing.split(\" \") if \n",
        "                      word in space_indicators['large']]\n",
        "words_overlap_large\n",
        "\n",
        "### could then take length as a fraction of all words\n",
        "len(words_overlap_small)/len(practice_listing.split(\" \"))\n",
        "len(words_overlap_large)/len(practice_listing.split(\" \"))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['COZY', 'LITTLE']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dL4Z__dxmW0"
      },
      "source": [
        "## Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vss2Co7B0Udu",
        "outputId": "cfc9b8a5-d836-44e9-e344-ebff59436dbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qhXJkndlxmW0",
        "outputId": "69750289-333c-444d-dfd3-e7e453d05017"
      },
      "source": [
        "## specify example\n",
        "example_for_tag = \"This is a chill apt next to the subway in LES Chinatown\"\n",
        "example_for_tag"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is a chill apt next to the subway in LES Chinatown'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er6FEQvM0xQp",
        "outputId": "28ab30a2-0075-443c-c789-378cfa5b4c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm7w2t3uxmW0",
        "outputId": "9beb1b36-c6ad-41b1-e0a9-73e13d303881"
      },
      "source": [
        "## try part of speech tagging using nltk\n",
        "tokens = word_tokenize(example_for_tag) # Generate list of tokens\n",
        "tokens_pos = pos_tag(tokens) # generate part of speech tags for those tokens\n",
        " \n",
        "## returns a list of tuples\n",
        "## first element in tuple is a word\n",
        "## second element in tuple is the part of speech\n",
        "for one_tok in tokens_pos:\n",
        "    print(one_tok)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('This', 'DT')\n",
            "('is', 'VBZ')\n",
            "('a', 'DT')\n",
            "('chill', 'NN')\n",
            "('apt', 'JJ')\n",
            "('next', 'JJ')\n",
            "('to', 'TO')\n",
            "('the', 'DT')\n",
            "('subway', 'NN')\n",
            "('in', 'IN')\n",
            "('LES', 'NNP')\n",
            "('Chinatown', 'NNP')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAZj2azExmW1",
        "outputId": "feaf6281-2500-4e31-e351-5afe356096be"
      },
      "source": [
        "## use list iteration to extract proper nouns (NNP)\n",
        "## i'm first checking if the second element in the tuple\n",
        "## is equal to NNP\n",
        "## if so, i'm returning the first element in the tuple (the \n",
        "## actual word)\n",
        "all_prop_noun = [one_tok[0] for one_tok in tokens_pos \n",
        "                if one_tok[1] == \"NNP\"]\n",
        "all_prop_noun"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LES', 'Chinatown']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJLMEuVuxmW1"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjH6a1EfxmW1"
      },
      "source": [
        "## modified from: https://twitter.com/dartmouth/status/1387488541844856838\n",
        "\n",
        "## tweet\n",
        "d_tweet = \"\"\"Dependents, partners, and household members of\n",
        "Dartmouth College students, staff, and faculty who are 18 or older are\n",
        "now eligible to sign up for COVID-19 vaccination clinics on May 5 and May 6.\n",
        "The deadline to sign up is 11:59 p.m. on April 29. These are in New Hampshire.\n",
        "\"\"\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MTlpG4WxmW2",
        "outputId": "32ec9df2-9916-4630-a3c6-1c69d9ba0c68"
      },
      "source": [
        "spacy_dtweet = nlp(d_tweet)\n",
        "for one_tok in spacy_dtweet.ents:\n",
        "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entity: Dartmouth College; NER tag: ORG\n",
            "Entity: 18 or older; NER tag: DATE\n",
            "Entity: May 5; NER tag: DATE\n",
            "Entity: May 6; NER tag: DATE\n",
            "Entity: 11:59 p.m.; NER tag: TIME\n",
            "Entity: April 29; NER tag: DATE\n",
            "Entity: New Hampshire; NER tag: GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLwMCua2xmW2"
      },
      "source": [
        "## try a couple variations\n",
        "## eg removing college, NH compared to New Hampshire\n",
        "## capitalize faculty"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWGvV8AMxmW2"
      },
      "source": [
        "## Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDtsGjSxmW2"
      },
      "source": [
        "### Using the default scorer on a few example phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isPUQbr4xmW2",
        "outputId": "0ecbdc3e-08a4-48cc-9ae5-0319090adbda"
      },
      "source": [
        "## initialize a scorer\n",
        "sent_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "## score one listing\n",
        "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
        "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
        "sentiment_example"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.4215, 'neg': 0.0, 'neu': 0.641, 'pos': 0.359}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwOXIzUixmW3"
      },
      "source": [
        "## adding phrase with word terrible and score\n",
        "practice_listing_2 = \"NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW.\"\n",
        "sentiment_example_2 = sent_obj.polarity_scores(practice_listing_2)\n",
        "\n",
        "## adding phrase about rats; bad but might not be in scoring dictionary\n",
        "practice_listing_3 = \"NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH.\"\n",
        "sentiment_example_3 = sent_obj.polarity_scores(practice_listing_3)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaJRgJmixmW3",
        "outputId": "7ffac012-5e7f-45a1-d945-1a219a4ce7db"
      },
      "source": [
        "## summarize all 3\n",
        "print(\"String: \" + practice_listing + \" scored as:\\n\" + str(sentiment_example))\n",
        "print(\"String: \" + practice_listing_2 + \" scored as:\\n\" + str(sentiment_example_2))\n",
        "print(\"String: \" + practice_listing_3 + \" scored as:\\n\" + str(sentiment_example_3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "String: NICE AND COZY LITTLE APT AVAILABLE scored as:\n",
            "{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}\n",
            "String: NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW. scored as:\n",
            "{'neg': 0.257, 'neu': 0.531, 'pos': 0.212, 'compound': -0.1513}\n",
            "String: NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH. scored as:\n",
            "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.4215}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u1JWsyhxmW4"
      },
      "source": [
        "### Updating the dictionary with manually-added words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px3TD-DsxmW4",
        "outputId": "d64e8b36-af32-4555-f020-1b0f03aacf0d"
      },
      "source": [
        "## lexicon is a dictionary where the key\n",
        "## is the word\n",
        "## the value is the score (negative = negative)\n",
        "## here, i'm benchmarking the negativity of the\n",
        "## rodents to the negativity of the word aversion\n",
        "sent_obj.lexicon['aversion']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzbulW5LxmW4",
        "outputId": "ad27a834-236c-4b41-c813-2cd4ab2c4dc9"
      },
      "source": [
        "## create a dictionary with \n",
        "## negative scores for pests\n",
        "pest_words = {\n",
        "    'rat': -1.9,\n",
        "    'rats': -1.9,\n",
        "    'mice': -1.9,\n",
        "    'mouse': -1.9,\n",
        "    'roach': -1.9,\n",
        "    'cockroach': -1.9\n",
        "}\n",
        "\n",
        "\n",
        "## initiate new sentiment object\n",
        "## so that we don't alter old one\n",
        "## use.update to add new words\n",
        "new_si = SentimentIntensityAnalyzer()\n",
        "new_si.lexicon.update(pest_words)\n",
        "\n",
        "## try re-scoring the third example\n",
        "## see negative\n",
        "print(\"After lexicon update: \" + practice_listing_3 + \" scored as:\\n\" + \\\n",
        "      str(new_si.polarity_scores(practice_listing_3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After lexicon update: NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH. scored as:\n",
            "{'neg': 0.228, 'neu': 0.551, 'pos': 0.22, 'compound': -0.0258}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}